#!/usr/bin/env python

import os, sys
import logging
import distutils.sysconfig

# ACISS is still using Python 2.6.6
from optparse import OptionParser

# include autoperf in the Python's search path
abs_path  = os.path.realpath(sys.argv[0])
cur_dir   = os.path.dirname(abs_path)
root_dir  = os.path.dirname(cur_dir)
version   = 'python'+distutils.sysconfig.get_python_version()
site_dirs = [
    root_dir,
    os.path.join(root_dir, 'lib',   version, 'site-packages'),
    os.path.join(root_dir, 'lib64', version, 'site-packages'),
    ]

for d in site_dirs:
    pkg_dir = os.path.join(d, 'autoperf')
    if os.path.isdir(pkg_dir) and d not in sys.path:
        sys.path.append(d)

# command line parsing

parser = OptionParser()
parser.add_option("-C", "--Config",
                  action="store", dest="cfgfile",
                  help="Specify a config file. If not specified or file "
                  "does not exist, search for .autoperf.cfg, autoperf.cfg, "
                  "~/.autoperf.cfg in order")
parser.add_option("-a", "--all",
                  action="store_true", dest="runall", default=True,
                  help="Run each experiment once. Default if no option is "
                  "given. Has no effect if '-e' is given")
parser.add_option("-e", "--exp",
                  action="append", dest="exps", metavar="EXP[@NUM]",
                  help="Run experiment EXP NUM times. This option can be "
                  "used several times and experiments will be executed in "
                  "the order they appear. [default: NUM=1]")
parser.add_option("-b", "--block",
                  action="store_true", dest="block", default=False,
                  help="Instead of exiting immediately after submitting the"
                  "experiment to the batch system, now block until the job "
                  "is finished [default: %default]")
parser.add_option("-c", "--check",
                  action="store_const", const="check", dest="mode",
                  help="When used with '-a', '-e' or '-i', show the status ("
                  "Unknown, Queueing, Running or Finished) of those experiments "
                  "instead of running them")
parser.add_option("-y", "--analyze",
                  action="store_const", const="analyze", dest="mode",
                  help="When used with '-a', '-e' or '-i', analyze those "
                  "experiments data instead of running them. The experiment must "
                  "be in 'Finished' state")
parser.add_option("-i", "--insname",
                  action="append", type="string", dest="insnames",
                  metavar="INSTANCE", default=None,
                  help="Use with '-c' or '-y' to specify the instance name of "
                  "the experiment. Check or analyze all instances if not specified. "
                  "This option can be specified multiple times")

(options, args) = parser.parse_args()

# now run the experiments

from autoperf.utils      import config
from autoperf.experiment import *

def parse_config():
    cfgfiles = ['.autoperf.cfg',
                'autoperf.cfg',
                os.path.expanduser('~/.autoperf.cfg')]

    if options.cfgfile is not None:
        cfgfiles.insert(0, options.cfgfile)

    for cfgfile in cfgfiles:
        try:
            config.parse(cfgfile)
        except:
            print "invalid, trying next option..."
        else:
            break

    if not config.done:
        print "*** Can not find any valid config file. Abort"
        exit(1)

def get_experiment_list(allow_dup=True):
    """
    Get the list of experiment we need to concern with
    """
    global options
    if options.exps is None:
        return config.get("Main.Experiments").split()
    else:
        exps = [ ]
        for expnum in options.exps:
            try:
                exp, num = expnum.split('@')
                if not allow_dup:
                    num = 1
            except ValueError:
                exp = expnum
                num = 1

            for i in range(int(num)):
                exps.append(exp)

        return exps

def run_experiments():
    exps = get_experiment_list()
    for exp in exps:
        experiment = Experiment(exp)
        experiment.setup()
        experiment.run(options.block)

        if (options.block):
            experiment.analyze()

        experiment.cleanup()

def check_an_experiment(expname, insnames=None):
    """
    Check and return the status of all the instances of experiment
    `expname`. If `insname` is not None, check only those instances

    Args:
      expname (string): Name of an experiment
      insnames  (list): List of instances need to check

    Returns:
      list: List of instance status
    """
    logging.disable(logging.CRITICAL)

    experiment = Experiment(expname, None)
    experiment.setup()

    stats = experiment.get_status()

    experiment.cleanup()

    logging.disable(logging.NOTSET)

    if insnames is None:
        return stats
    else:
        return [stat for stat in stats if stat[0]['insname'] in insnames]

def is_finished(instance):
    """
    Check whether an experiment instance is finished. The instance
    could have several iterations because of the partitioner, so it is
    considered as 'Finished' only if all iterations are done.

    Args:
      instance (list): List of iteration status

    Returns:
      bool: Whether the instance is finished or not
    """
    for iteration in instance:
        if iteration['status'] != 'Finished':
            return False

    return True

def check_experiments(insnames):
    fmtstr = "{0:20} {1:30} {2:15} {3:10}"

    print "*** Experiment status:"
    print fmtstr.format("Experiment", "Instance", "Job ID", "Status")
    print "----------------------------------------------------------------------------"

    exps = get_experiment_list(False)
    for exp in exps:
        for instance in check_an_experiment(exp, insnames):
            # all iterations of an instance shall have same
            # experiment name and instance name
            expname = instance[0]['expname']
            insname = instance[0]['insname']

            if is_finished(instance):
                print fmtstr.format(expname, insname, "-", "Finished")
            else:
                for iteration in instance:
                    print fmtstr.format(expname, insname,
                                        iteration['jobid'],
                                        iteration['status'])
                    # don't bother show expname and insname for
                    # the following iterations
                    expname = ""
                    insname = ""

def analyze_an_experiment(expname, insnames):
    for instance in check_an_experiment(expname, insnames):
        insname = instance[0]['insname']
        if is_finished(instance):
            print "*** Analyzing %s %s ..." % (expname, insname)
            experiment = Experiment(expname, insname)
            experiment.setup()
            experiment.analyze()
            experiment.cleanup()
        else:
            print "*** Experiment %s %s is not finished yet, ignore" % (expname, insname)

def analyze_experiments(insnames):
    exps = get_experiment_list(False)
    for exp in exps:
        analyze_an_experiment(exp, insnames)

parse_config()

if options.mode is None:
    run_experiments()
elif options.mode == 'check':
    check_experiments(options.insnames)
elif options.mode == 'analyze':
    analyze_experiments(options.insnames)
